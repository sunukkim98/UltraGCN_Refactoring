{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "## Template data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/UltraGCN/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_path, train=True):\n",
    "        self.train = train\n",
    "        self.data = torchvision.datasets.MNIST(root=data_path,\n",
    "                                               train=self.train,\n",
    "                                               transform=torchvision.transforms.ToTensor(),\n",
    "                                               download=True)\n",
    "\n",
    "        _, self.width, self.height = self.data[0][0].shape\n",
    "        self.in_dim = self.width * self.height\n",
    "        self.out_dim = len(self.data.classes)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.__len__()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data.__getitem__(idx)\n",
    "\n",
    "    def get_features(self):\n",
    "        return self.data.train_data if self.train else self.data.data.float()\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self.data.train_labels if self.train else self.data.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1076)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../datasets/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:01<00:00, 5546802.10it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../datasets/MNIST/raw/train-images-idx3-ubyte.gz to ../datasets/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1076)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../datasets/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 145338.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../datasets/MNIST/raw/train-labels-idx1-ubyte.gz to ../datasets/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1076)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../datasets/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:01<00:00, 1399972.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../datasets/MNIST/raw/t10k-images-idx3-ubyte.gz to ../datasets/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1076)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 2605022.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../datasets/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 인스턴스 생성 및 테스트\n",
    "data_path = \"../datasets\" # 데이터 경로 지정\n",
    "datasets = MyDataset(data_path, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([1, 28, 28]), Label: 5\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋의 첫 번째 샘플 가져오기\n",
    "sample = datasets[0]\n",
    "print(f\"Image shape: {sample[0].shape}, Label: {sample[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset text (/root/.cache/huggingface/datasets/reczoo___text/reczoo--AmazonBooks_m1-3655901e653150e2/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "100%|██████████| 2/2 [00:00<00:00, 938.43it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"reczoo/AmazonBooks_m1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    }
   ],
   "source": [
    "data_path = '../datasets/AmazonBooks_m1'\n",
    "dataset.save_to_disk(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_path, hugging_path, train=True):\n",
    "        self.train = train\n",
    "        self.data = load_dataset(hugging_path, # huggingface path\n",
    "                                 split=\"train\" if train else \"test\")\n",
    "        self.data.save_to_disk(data_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 특정 파일만 huggingface에서 다운로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading train.txt: 100%|██████████| 14.1M/14.1M [00:00<00:00, 43.0MB/s]\n",
      "Downloading train.txt: 100%|██████████| 6.90M/6.90M [00:01<00:00, 6.44MB/s]\n",
      "Downloading train.txt: 100%|██████████| 4.63M/4.63M [00:00<00:00, 63.8MB/s]\n",
      "Downloading test.txt: 100%|██████████| 3.85M/3.85M [00:00<00:00, 96.0MB/s]\n",
      "Downloading test.txt: 100%|██████████| 1.99M/1.99M [00:00<00:00, 46.8MB/s]\n",
      "Downloading test.txt: 100%|██████████| 1.37M/1.37M [00:00<00:00, 65.2MB/s]\n",
      "Downloading train_data.json: 100%|██████████| 4.44M/4.44M [00:00<00:00, 30.0MB/s]\n",
      "Downloading test_data.json: 100%|██████████| 565k/565k [00:00<00:00, 35.3MB/s]\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "dataset_list = [\"AmazonBooks_m1\",\n",
    "                \"Yelp18_m1\",\n",
    "                \"Gowalla_m1\"]\n",
    "\n",
    "for _ in dataset_list:\n",
    "    dataset = hf_hub_download(\"reczoo/\" + _, \n",
    "                       filename=\"train.txt\",\n",
    "                       repo_type=\"dataset\",\n",
    "                       cache_dir=\"../datasets/\" + _)\n",
    "\n",
    "for _ in dataset_list:\n",
    "    dataset = hf_hub_download(\"reczoo/\" + _, \n",
    "                       filename=\"test.txt\",\n",
    "                       repo_type=\"dataset\",\n",
    "                       cache_dir=\"../datasets/\" + _)\n",
    "\n",
    "dataset = hf_hub_download(\"reczoo/Movielens1M_m1\", \n",
    "                       filename=\"train_data.json\",\n",
    "                       repo_type=\"dataset\",\n",
    "                       cache_dir=\"../datasets/Movielens1M_m1\")\n",
    "\n",
    "dataset = hf_hub_download(\"reczoo/Movielens1M_m1\", \n",
    "                       filename=\"test_data.json\",\n",
    "                       repo_type=\"dataset\",\n",
    "                       cache_dir=\"../datasets/Movielens1M_m1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Conda (UltraGCN)",
   "language": "python",
   "name": "ultragcn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
